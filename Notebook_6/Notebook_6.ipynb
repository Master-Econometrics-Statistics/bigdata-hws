{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classifier\n",
        "\n",
        "## Performing Pre-Processing and Sequential filtering on RGB Data for Topic Identification\n",
        "\n",
        "### Summary: \n",
        "\n",
        "#### 1. Virtual Environment Set-Up\n",
        "#### 2. Database Inspection\n",
        "#### 3. Pipeline Building\n",
        "##### 3.1 Engineering of a Lenet_like class\n",
        "##### 3.2 Engineering of a Lenet_like model\n",
        "#### 4. Model Fitting\n",
        "#### 5. Cross-Validation\n",
        "#### 6. Sensitivity Analysis"
      ],
      "metadata": {
        "id": "0BKzh82jgR27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount the virtual environment on a Google Drive folder"
      ],
      "metadata": {
        "id": "CmPOcH08gH0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diDc6LA-ABob",
        "outputId": "fd2c7048-79c9-40cf-8eb6-b686186d9073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and load packages from the keras library"
      ],
      "metadata": {
        "id": "z30NACf_hMpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AYD_C2WumJye"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, Flatten, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy\n",
        "import os\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC7Bu-qLdcoI"
      },
      "source": [
        "## Setup\n",
        "Before you start, please mount your google drive.\n",
        "For each exercise, create an output directory to store monitoring data and your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0QNnu_AgBBBR"
      },
      "outputs": [],
      "source": [
        "OUTDIR = \"/content/drive/My Drive/Classification\"\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.makedirs(OUTDIR)\n",
        "# First, check dimensions of the datasets !\n",
        "# shape of images in cifar10: (?, ?, ?)\n",
        "# shape of images in mnist: (?, ?)\n",
        "# shape of images in fashion_mnist: (?, ?)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# defining training and testing sets\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
        "x_train=x_train/1000.0\n",
        "x_train.shape\n",
        "x_test=x_test/1000.0\n",
        "x_test.shape"
      ],
      "metadata": {
        "id": "M7YGRS52helE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5777e5-3398-4cd5-8857-637eca3e7a03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining training and testing sets\n",
        "\n",
        "(x_train_2, y_train_2), (x_test_2, y_test_2) = tensorflow.keras.datasets.mnist.load_data()\n",
        "x_train_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-euBtQKuz3C",
        "outputId": "8768c95c-9af3-42c2-c60e-a5fbeeadc29a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "uAtJukMrij74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd447440-acef-4c51-ff7d-5712d55071d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "kbutaPuFjA1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a010a897-1fc7-47ce-ee57-6cdd0129fae4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape[3]"
      ],
      "metadata": {
        "id": "Jh4RARhgvVZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa91a57-446c-4cf3-f0a1-d505b8688dcf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_test[215])"
      ],
      "metadata": {
        "id": "1ZzClb4LsCx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "dc78b675-8e51-4a99-d5ef-05231162c192"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3f132cdbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVW0lEQVR4nO3df4hld3nH8fcz986d2fmxm+xuEpYYGrWBEqRGGYJFEasoqQhRKEH/kPwRXCkGKtg/Qgo1hf6hpSr+USxrE4zFGlN/YCihbRqE4D/RjY2baNoaQ8Qsa9Yfu9mZnZ25c2ee/nFP6Gw4zzMzZ+6Pmf1+XrDsnfO9557vnHufOfd+n/t9vubuiMiVb2LcHRCR0VCwixRCwS5SCAW7SCEU7CKFULCLFKK9m53N7Dbgi0AL+Ed3/0x2/06n49PT07Vto0wBNj2Wme14n/X19Ub9aLXip6ZBN+QKE70WV1ZW6Ha7tY2Ng93MWsDfA+8FXgJ+aGaPuPtPo32mp6dZWFiobRtlsK+uroZtGxsbYdvU1FTt9omJ+A3SuXPnG/XjyJEjYdvkZPa0NTmP+uux30TBfvLkyXCf3byNvxV43t1fcPcu8BBw+y4eT0SGaDfBfj3wy00/v1RtE5E9aFef2bfDzI4DxyF+Gywiw7ebK/tp4IZNP7+u2nYZdz/h7gvuvtDpdHZxOBHZjd0E+w+Bm8zs9WbWAT4MPDKYbonIoDV+G+/uPTO7G/h3+qm3B9z9J00fr0laq6l2O/61L168uOPHm5mZadSP5eXlsG12djZs63TmGx2vmUE/L02zLsoY7NauPrO7+6PAowPqi4gMkb5BJ1IIBbtIIRTsIoVQsIsUQsEuUoihf4Nuu0Y5EabVaoVt0aw8iFNl2eQZ97gt2y9LAc7Oxqm+bFJOM/Hzkj1ncSo1e56z9JoKo27WJFWtK7tIIRTsIoVQsIsUQsEuUggFu0gh9sxo/KA1nViTTcONHvPChQvhPlnpqayP2X5ra2th26Br/G1sxPt1u3Efo8lG2SSkzCgnSu0Vg85Q6couUggFu0ghFOwihVCwixRCwS5SCAW7SCH2ReotSrtkqYlhTKyZnJys3T43Nxfuk01oyfq43ouXjbp0aSVsa1KuO0tr9Xpxmu/ixbiGXpRiyybxZGm5QT+fJabydGUXKYSCXaQQCnaRQijYRQqhYBcphIJdpBC7Sr2Z2YvAIrAO9Nx9Ibu/u4cztrLaaYOvqzY6vV4vbNtYj9Nr7an4qel2u2Hbykr9TLQobQjQasXnN5th10vSg9FsuYmJOG2YLXk1MdEsVVZiii0yiDz7H7v7bwbwOCIyRPv3kikiO7LbYHfgP8zsKTM7PogOichw7PZt/Dvc/bSZXQs8Zmb/7e5PbL5D9UfgODT7KqeIDMauruzufrr6/yzwHeDWmvuccPcFd1/IBolEZLgaB7uZzZrZ/Ku3gfcBzw6qYyIyWLt5G38d8J0qtdEG/tnd/y3bYW1tjbNnz9Z3JJnxFL39n5yMi0NOTsaP12rFbVmab2OjPtV07ty5cJ/19XiJp07ysSZ7F5TNAFtaqp9l127HS15NtuNj9ZL0YLafB8s1dbtxKq/djgtYZstyZdm1JrPlrtR0XeNgd/cXgDcPsC8iMkRKvYkUQsEuUggFu0ghFOwihVCwixRipAUnzSxcSy1bL21paal2e5ZCy1I1WZov2y9Ky2XrsjWdsbeepLwgTg15MNsse7wsHZYloSydqVi/Z5bW6ibnMUtFZmnWSNNipfs5Lacru0ghFOwihVCwixRCwS5SCAW7SCFGOhrfbrc5evRobVs2Ch6N1GejyNmyS9kI+cVg5B/i0X+z7G9mPAq+thbXp0tHhIlHraMR8qyG20TS/2waSZO6cNm5z7IkG8n5mJ6OJxRFx8uO1WrFk4b280i9ruwihVCwixRCwS5SCAW7SCEU7CKFULCLFGLkE2Gi9MTc3Fy434EDB2q3r6zESwmdP38+bFtdjZdPWve4ZlxUjy1LJ2VpHIiPtZ4sG0VyvCg1lKUps/xaOmEk2THaL8tOZamrLL2ZnePoubn22mvCfebn58O2/UxXdpFCKNhFCqFgFymEgl2kEAp2kUIo2EUKsWXqzcweAD4AnHX3N1XbDgPfAG4EXgTucPd4DaRdilIrs7Oz4T7Z0lCLi3G9u+Xl5bAtquOW1XdL00lhy1YzqHa+pNFQaq6l3ahvzFZjypdqitOU2X7Ra2dpKX6eZ2bi11WrtfO051ZGNVtuO1f2rwC3vWbbPcDj7n4T8Hj1s4jsYVsGe7Xe+u9es/l24MHq9oPABwfcLxEZsKaf2a9z9zPV7V/RX9FVRPawXX9d1t3dzMIPK2Z2HDgO8dLLIjJ8Ta/sL5vZMYDq//pF1wF3P+HuC+6+EC0QISLD1zTYHwHurG7fCXx3MN0RkWHZTurt68C7gKNm9hLwaeAzwMNmdhfwC+COYXayiU4nXi7o6quvDttmZmbCtmiW3eLiYrhPtxvPsMuSb1k6ZiOZmRelAZum3pqm5aK2pumpTLY01Nxc/Qy29V5WCDR+ztrt+hmYMJzfbZC2DHZ3/0jQ9J4B90VEhkjfoBMphIJdpBAKdpFCKNhFCqFgFynESAtONjXoNE5WIDIqbgnxNwCnprIZdvHacVnBzLRAZANNz1VWzHFjI5uJtvN+NJ39lfej/njZayBbg296em+n1zK6sosUQsEuUggFu0ghFOwihVCwixRCwS5SiH2ReotkqZphzECK0jXZTLlOUrCjuxqn17KZdIsX4oKZG8E5SeqLpOuoZUU9l5bitGL01Lg3S+VlWblesi7e0lL9eZyamg736a6uhm0rk3HITE/Hj7kX6MouUggFu0ghFOwihVCwixRCwS5SiH09Gp8ZxpI6Hq135PGx2q34FLdn4tppnWRyTVaS+9y5167n0dddiWuuZTXtut14ZNqT/aLlt1rJ+Vhfj0fV19aaTQzq9er3y0b+V1fjCUq9pHZd59r4eZmYGG3mqLYPIzmKiIydgl2kEAp2kUIo2EUKoWAXKYSCXaQQ21n+6QHgA8BZd39Tte0+4GPAr6u73evujw6rk3tFOJek4YScMJVHXvvt4KH6JY0AvFe/dNEr63HqqrcR9+NSUifPk/2iFZkmknM1kSzjFC1rBXkaLVpia2MjfrzsWKvJJJksTZnVNtxLqbevALfVbP+Cu99S/bviA11kv9sy2N39CaD+mxoism/s5jP73WZ2ysweMLN4WVQR2ROaBvuXgDcCtwBngM9FdzSz42Z20sxO5ssXi8gwNQp2d3/Z3de9/+XoLwO3Jvc94e4L7r7Q6cTf9xaR4WoU7GZ2bNOPHwKeHUx3RGRYtpN6+zrwLuComb0EfBp4l5ndAjjwIvDxIfZx7wgLqyW7BKmfrXZMkzHJbDPbqJ85Npksd3Tk6NGwbTlJNb3yyithW5QOW0lmlE0mqbemsxjj3ZLrXJIKu3TpUth2/nx8PrKad0OYoFlry2B394/UbL5/CH0RkSHSN+hECqFgFymEgl2kEAp2kUIo2EUKccUWnNw7ms1oytIxa8k3EddWlmu3d9rxUz07Fy/xNHfoYNg2Px/Pvlteru/H4oV4WavVht+wTIs5hg3NZiP2kqKY2XJYV111KGyLZsQNejacruwihVCwixRCwS5SCAW7SCEU7CKFULCLFEKptz3Ks7XIli/G+wVrm03OJOuQJeuvZTnAmZmZsG16un6W14ED8T7nz58L27LZZtn6axal0dLJiEljw5To0lL8nGXFKAdJV3aRQijYRQqhYBcphIJdpBAKdpFCaDR+B8JpCc3KzMUjxeQju6vJyHR0wFY7ru9GUp8uq3eXmQgec3Y2Ho3vdOI+Li7Gk0yiSTcAKyv15yobwY/X+cprCmYTVy5cuBC2HTxYP9koOx9N6MouUggFu0ghFOwihVCwixRCwS5SCAW7SCG2s/zTDcBXgevo53VOuPsXzeww8A3gRvpLQN3h7vFMhivBgGuC9ZKlkJbOnw/bVi8lSygF9diSMm2QTLpJ04p5445lyz9dffVVYdv8/FzYFk1AWVqKa+GtrMRLXq2vJym75PURpQABFhfr03JHjhyJj9XAdq7sPeBT7n4z8DbgE2Z2M3AP8Li73wQ8Xv0sInvUlsHu7mfc/UfV7UXgOeB64HbgwepuDwIfHFYnRWT3dvSZ3cxuBN4CPAlc5+5nqqZf0X+bLyJ71LaD3czmgG8Bn3T3yz5keP97grUfWMzsuJmdNLOT3YZ1wUVk97YV7GY2ST/Qv+bu3642v2xmx6r2Y8DZun3d/YS7L7j7QqfTGUSfRaSBLYPdzIz+euzPufvnNzU9AtxZ3b4T+O7guycig7KdWW9vBz4KPGNmT1fb7gU+AzxsZncBvwDuGE4X944o0ZQl5DaSVM3qxTj9s3Yprlm2HtSZA2hPtmq391bjdNLqxXhG2dRsvDRUukbVgFlyrCxlFy27dOBAfY08gEtJavNicq4uXoxn3/WS5+x8kGbNavxlbZEtg93dv0/8On/Pjo8oImOhb9CJFELBLlIIBbtIIRTsIoVQsIsUQgUnByFJC2VFCNfXe2FbNHsNoJtkvNaDGWy9tfjbi4vJskvWqk/lAXTSZYsGm5ZrWNMzTNlFy1MBZF/+ylJeU1NxUclz5+JzvLJSn+r77W9/G+4T9T97venKLlIIBbtIIRTsIoVQsIsUQsEuUggFu0ghlHrbAQ/zP3G6o9WO/55OtOLTn00oayfpsI0g9bbei9N8G7244OSF38Xpn0NHjoZtk2Fqq1lKbrClPnPROnUAU1NxWu7w4cNhW7ZuW7RW3aVkTb/FxfoZk9HzD7qyixRDwS5SCAW7SCEU7CKFULCLFEKj8Tuy8yp0ZvHf03ZnKmxbS+rT4fGIa6tVf7xsgsTGRjxS312Oa+FdTEatD15zTe32iXazCsNZDbpM9HsP+vEgPvcAhw7V18IDOHjwYO32bDQ+mjyjiTAiomAXKYWCXaQQCnaRQijYRQqhYBcpxJapNzO7Afgq/SWZHTjh7l80s/uAjwG/ru56r7s/OqyOXolak3EaaqIdT5wwi5cn8mAihCVpMsgmT8SpnNUkLbf8Sn3/Z666Oj5WK/6ds5RSplmKLUulJns1nK0T9TGrdxfVoGslk6S2k2fvAZ9y9x+Z2TzwlJk9VrV9wd3/bhuPISJjtp213s4AZ6rbi2b2HHD9sDsmIoO1o8/sZnYj8BbgyWrT3WZ2ysweMLP4/ZmIjN22g93M5oBvAZ909wvAl4A3ArfQv/J/LtjvuJmdNLOT3W5cu1xEhmtbwW5mk/QD/Wvu/m0Ad3/Z3dfdfQP4MnBr3b7ufsLdF9x9ISu+LyLDtWWwW3+o8H7gOXf//Kbtxzbd7UPAs4PvnogMynZG498OfBR4xsyerrbdC3zEzG6hn6d4Efj4UHq4hzQoQZfUrYOJJPU2ORUvT9TrxrOhVrtrtduT0mTpzLz1bIZd8ntfCmqkkRxrNknL2UScUtorhpGWi0QptizVuJ3R+O9T/zpXTl1kH9E36EQKoWAXKYSCXaQQCnaRQijYRQqhgpPDlqRcsnRSayouRtlajp+21sR67fa1JPXmSXotyZSR/nLr9SnAlSglB7STVOT0fH1Rxv1ilGm5iK7sIoVQsIsUQsEuUggFu0ghFOwihVCwixRCqbdByGoaZqm3ZL9WJ571ZklhRmO1dvvERHyw3kbSySRlRyt+TA9+OUsesLeWFDfJ8lMN121LHnDAj7fF0YLDNS2yGdGVXaQQCnaRQijYRQqhYBcphIJdpBAKdpFCKPU2EHGqxixOn2SJlYlkza6JyTj1FkrSa1nBySzlZRa/fDoH6lOH7aSQ5tTMbNyPgafXyqMru0ghFOwihVCwixRCwS5SCAW7SCG2HI03s2ngCWCquv833f3TZvZ64CHgCPAU8FF31zKtO5HWp4v/Drc7cX06orp2yfpPG8kaVZ3kWDNz82Hbgbn6kXVrx5mEifbeX+JpP9vOlX0VeLe7v5n+8sy3mdnbgM8CX3D33wfOAXcNr5sisltbBrv3LVU/Tlb/HHg38M1q+4PAB4fSQxEZiO2uz96qVnA9CzwG/Bw47+696i4vAdcPp4siMgjbCnZ3X3f3W4DXAbcCf7DdA5jZcTM7aWYnu119pBcZlx2Nxrv7eeB7wB8BV9n/f1/ydcDpYJ8T7r7g7gudTrwIgIgM15bBbmbXmNlV1e0DwHuB5+gH/Z9Wd7sT+O6wOikiu7ediTDHgAfNrEX/j8PD7v6vZvZT4CEz+xvgv4D7h9jP4mSTU9KloYIllGwizvO1k3TY7MFDYdv0gXjiSpRGa1pVzTQRZte2DHZ3PwW8pWb7C/Q/v4vIPqBv0IkUQsEuUggFu0ghFOwihVCwixTCBr3ETHows18Dv6h+PAr8ZmQHj6kfl1M/Lrff+vF77n5NXcNIg/2yA5uddPeFsRxc/VA/CuyH3saLFELBLlKIcQb7iTEeezP143Lqx+WumH6M7TO7iIyW3saLFGIswW5mt5nZ/5jZ82Z2zzj6UPXjRTN7xsyeNrOTIzzuA2Z21sye3bTtsJk9ZmY/q/6/ekz9uM/MTlfn5Gkze/8I+nGDmX3PzH5qZj8xsz+vto/0nCT9GOk5MbNpM/uBmf246sdfV9tfb2ZPVnHzDTPbWYEIdx/pP6BFv6zVG4AO8GPg5lH3o+rLi8DRMRz3ncBbgWc3bftb4J7q9j3AZ8fUj/uAvxjx+TgGvLW6PQ/8L3DzqM9J0o+RnhP6iwfOVbcngSeBtwEPAx+utv8D8Gc7edxxXNlvBZ539xe8X3r6IeD2MfRjbNz9CeB3r9l8O/3CnTCiAp5BP0bO3c+4+4+q24v0i6Ncz4jPSdKPkfK+gRd5HUewXw/8ctPP4yxW6cB/mNlTZnZ8TH141XXufqa6/SvgujH25W4zO1W9zR/6x4nNzOxG+vUTnmSM5+Q1/YARn5NhFHktfYDuHe7+VuBPgE+Y2TvH3SHo/2WneVGX3foS8Eb6awScAT43qgOb2RzwLeCT7n5hc9soz0lNP0Z+TnwXRV4j4wj208ANm34Oi1UOm7ufrv4/C3yH8VbeednMjgFU/58dRyfc/eXqhbYBfJkRnRMzm6QfYF9z929Xm0d+Tur6Ma5zUh17x0VeI+MI9h8CN1Ujix3gw8Ajo+6Emc2a2fyrt4H3Ac/mew3VI/QLd8IYC3i+GlyVDzGCc2L9AnP3A8+5++c3NY30nET9GPU5GVqR11GNML5mtPH99Ec6fw785Zj68Ab6mYAfAz8ZZT+Ar9N/O7hG/7PXXfTXzHsc+Bnwn8DhMfXjn4BngFP0g+3YCPrxDvpv0U8BT1f/3j/qc5L0Y6TnBPhD+kVcT9H/w/JXm16zPwCeB/4FmNrJ4+obdCKFKH2ATqQYCnaRQijYRQqhYBcphIJdpBAKdpFCKNhFCqFgFynE/wG1eUnQM4cOogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is defined as a four-dimensional vector (number_of_images, number_of_pixels_in_length, number_of_pixels_in_height, number_of_features_per_pixel)\n",
        "\n",
        "Here the number of features per pixels equals 3 and translates into the value of R, G and B."
      ],
      "metadata": {
        "id": "-7mBPt_2hrey"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-c46sXwd8Eh"
      },
      "source": [
        "# EX1: Lenet-like classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrk3AVtfLbS"
      },
      "source": [
        "## 1- Build model architecture\n",
        "The architecture has to be a CLASS called \"Lenet_like\".\n",
        "\n",
        "- I can define the width: number of filters in the first layer.\n",
        "- I can define the depth: number of conv layers.\n",
        "- I can specify the number of classes: output neurons.\n",
        "\n",
        "Lenet_like has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n",
        "\n",
        "Hidden dense layer will have 512 units."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a Lenet_like class with an __init__ function taking as inputs:\n",
        "- self: the object \n",
        "- width: the number of filters in the first layer\n",
        "- depth: the size of each kernels\n",
        "- drop: the dropout rate\n",
        "- n_classes: the number of classes in the dataset i.e. the ouptut dimension\n",
        "\n",
        "Very simply speaking, we consider an image of 32x32 pixels, the goal of the Convolutional Neural Network is to simplify the image in order to extract its core features.\n",
        "\n",
        "The extraction of core features results in the processing of the said-image through a *filter*.\n",
        "\n",
        "Algebraically, such filter is defined by a matrix, called a *kernel*. In particular, filtering an image consists in pre-multiplying the matrix of pixels in their RGB encoding by the kernel of interest, and apply that application to a reduced sub-region of the image e.g. the central element. \n",
        "\n",
        "By doing this, one not only mutes undesired features of the image, but it exarcerbates the remaining features by applying them to entire batches of pixels. The matricial product i.e. convolution is in fact at the heart of the lexicology surrounding the Network's Theory.\n",
        "\n",
        "Edge handling involved different methods to make the convolution possible when there is no physical pixels neighbouring the ones that are being filtered or convoluted by the kernel. \n",
        "\n",
        "One can extend the image with as many white pixels as necessary to create an artificial neighbouring of the pixels at the edge. \n",
        "\n",
        "Alternatively, one can also wrap the image in order to tie together pixels positioned at opposite edges of the image. \n",
        "\n",
        "Mirroring is another technique used, and assume that the image is symmetric. \n",
        "\n",
        "One can also discard pixels with no neighbours, by cropping the image, or even adding a constant for such edge-pixels. \n",
        "\n",
        "\n",
        "The size of the kernel is in fact the size of the window of neighbours applied to each pixel, the larger the window, the more difficult it is for each pixels to find enough neigbhours. It actually pulls edges towards the center of the image and calls for eventually unnecessary croping, mirroring or wrapping pre-processing. \n",
        " \n",
        "\n",
        "Empirics guiding the practionner often calls for an increasing number of filters at each layer of the network. Learning from a raw image needs a lot of processing whereas end-processing can benefits from more filters without using as much RAM and GPU space.\n",
        "\n"
      ],
      "metadata": {
        "id": "1MuTt5ecjx81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jyZwILwynX9Q"
      },
      "outputs": [],
      "source": [
        "# first model\n",
        "# Lenet-like\n",
        "class Lenet_like:\n",
        "  \"\"\"\n",
        "  Lenet like architecture.\n",
        "  \"\"\"\n",
        "  def __init__(self, width, depth, drop, n_classes):\n",
        "    \"\"\"\n",
        "    Architecture settings.\n",
        "\n",
        "    Arguments:\n",
        "      - width: int, first layer number of convolution filters.\n",
        "      - depth: int, number of convolution layer in the network.\n",
        "      - drop: float, dropout rate.\n",
        "      - n_classes: int, number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    self.width = width\n",
        "    self.depth = depth\n",
        "    self.drop = drop\n",
        "    self.n_classes = n_classes\n",
        "  \n",
        "  def __call__(self, X):\n",
        "    \"\"\"\n",
        "    Call classifier layers on the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    for k in range(self.depth):\n",
        "      # Padding refers to the edge-handling techniques aforementioned . \n",
        "      # When the value of the parameter is set to \"VALID\", it means that there\n",
        "      # is no padding of zeros on the boudary of the image i.e. loss of data.\n",
        "      # The opposite happens when the value is set to \"SAME\".\n",
        "\n",
        "      # First Layer (tensor collected raw data)\n",
        "\n",
        "      Y = tensorflow.keras.layers.Conv2D(filters=self.width, kernel_size=self.depth, padding=\"same\", activation=\"relu\",\n",
        "                                         input_shape=(32, 32, 3))(X)\n",
        "\n",
        "      # Second Layer\n",
        "\n",
        "      Y = tensorflow.keras.layers.Conv2D(filters=self.width, kernel_size=self.depth, padding=\"same\", activation=\"relu\")(Y)\n",
        "\n",
        "\n",
        "      # First Max Pooling Layer\n",
        "\n",
        "      Y = tensorflow.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid')(Y)\n",
        "\n",
        "      # Third Layer\n",
        "\n",
        "      Y = tensorflow.keras.layers.Conv2D(filters=self.width*2, kernel_size=self.depth, padding=\"same\", activation=\"relu\")(Y)\n",
        "\n",
        "      # Fourth Layer\n",
        "      Y = tensorflow.keras.layers.Conv2D(filters=self.width*2, kernel_size=self.depth, padding=\"same\", activation=\"relu\")(Y)\n",
        "\n",
        "      # Second Max Pooling Layer\n",
        "      Y = tensorflow.keras.layers.MaxPool2D(pool_size=2, strides=2, padding=\"valid\")(Y)\n",
        "\n",
        "      # Flattening Layer\n",
        "      Y = tensorflow.keras.layers.Flatten()(Y)\n",
        "\n",
        "      # Droput Layer\n",
        "      Y = tensorflow.keras.layers.Dropout(self.drop, noise_shape=None, seed=None)(Y)\n",
        "\n",
        "\n",
        "      # Adding the first fully connected layer\n",
        "      Y = tensorflow.keras.layers.Dense(units=self.width*4, activation=\"relu\")(Y)\n",
        "\n",
        "\n",
        "      # Output Layer\n",
        "      Y = tensorflow.keras.layers.Dense(units=self.n_classes, activation=\"softmax\")(Y)\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8OPc7nifmTm"
      },
      "source": [
        "## 2- A function to create a model with Lenet_like architecture\n",
        "I want the model to be able to fit on the following datasets:\n",
        "\n",
        "- mnist\n",
        "- fashion-mnist\n",
        "- cifar-10\n",
        "\n",
        "Create a function called \"make_lenet_model\" that take the name of one of these dataset as a str.\n",
        "It returns a keras Model object.\n",
        "It should obviously take all arguments to init the Lenet_like architecture.\n",
        "Arguments other than the dataset might have default values.\n",
        "I want to be able to monitor the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cFbWjtAB_BDi"
      },
      "outputs": [],
      "source": [
        "def make_lenet_model(dataset,\n",
        "                     width=32,\n",
        "                     depth=2,\n",
        "                     drop=0.25,\n",
        "                     n_classes=1):\n",
        "  \"\"\"\n",
        "  Create a Lenet model adapted to the dimensions of a given dataset.\n",
        "  \"\"\"\n",
        "  if dataset == \"cifar10\":\n",
        "    # dimensions of input are: (?, ?, ?)\n",
        "    X = Input(batch_shape=(x_train.shape))\n",
        "  elif dataset == \"mnist\" or dataset == \"fashion_mnist\":\n",
        "    # dimensions of input are: (?, ?)\n",
        "    X = Input(batch_shape=(x_train_2.shape))\n",
        "  else:\n",
        "    raise NotImplementedError(\"Model not implemented for datastet {}\".format(dataset))\n",
        "  \n",
        "  Y = Lenet_like(width, depth, drop, n_classes)\n",
        "  \n",
        "  model = Y.__call__(x_train)\n",
        "  # Remember I wanna monitor accuracy\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_lenet_model(\"cifar10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "9ndcnwhC1LYa",
        "outputId": "df71528f-579b-46b8-f334-eb0011cc182c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-67fbb44084eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_lenet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-b6fbe3331337>\u001b[0m in \u001b[0;36mmake_lenet_model\u001b[0;34m(dataset, width, depth, drop, n_classes)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLenet_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m# Remember I wanna monitor accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sparse_categorical_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e68c52af96c9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       Y = tensorflow.keras.layers.Conv2D(filters=self.width, kernel_size=self.depth, padding=\"same\", activation=\"relu\",\n\u001b[0;32m---> 36\u001b[0;31m                                          input_shape=(32, 32, 3))(X)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m# Second Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"conv2d_1\" (type Conv2D).\n\nOOM when allocating tensor with shape[50000,32,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(50000, 32, 32, 3), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm0iDMSKgMF3"
      },
      "source": [
        "## 3- Create a fitting function\n",
        "\n",
        "I want a function \"fit_model_on\" with the following arguments:\n",
        "\n",
        "- dataset: str name of the dataset\n",
        "- epochs: number of times you fit on the entire training set\n",
        "- batch_size: number of images to average gradient on\n",
        "\n",
        "The function must create a Lenet model and fit it following these parameters.\n",
        "The function should:\n",
        "\n",
        "- fit the model, obviously\n",
        "- store the model architecture in a .json file in your output directory\n",
        "- store the model's weights in a .h5 file in your output directory\n",
        "- store the fitting metrics loss, validation_loss, accuracy, validation_accuracy under the form of a plot exported in a png file.\n",
        "\n",
        "Run your fitting function of course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AS61Zmb5JeTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "7c36aedb-cc9b-40c2-f171-f1d0830ad9bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-94d21951f4a0>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    x_train ...\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def fit_model_on(dataset,\n",
        "                 epochs=100,\n",
        "                 batch_size=32,\n",
        "                 n_classes=10):\n",
        "  \n",
        "  model_filename = \"lenet_{}.json\".format(dataset)\n",
        "  weight_filename = \"lenet_{}_weights.h5\".format(dataset)\n",
        "  lossplot_filename = \"lenet_{}_loss.png\".format(dataset)\n",
        "  accplot_filename = \"lenet_{}_accuracy.png\".format(dataset)\n",
        "  \n",
        "  # create your model and call it on your dataset\n",
        "  model = ...\n",
        "  # create a Keras ImageDataGenerator to handle your dataset\n",
        "  datagen = ...\n",
        "  \n",
        "  if dataset == \"cifar10\":\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  elif dataset == \"mnist\":\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  elif dataset == \"fashion_mnist\":\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  else:\n",
        "    raise NotImplementedError(\"Model not implemented for datastet {}\".format(dataset))\n",
        "  \n",
        "  # Convert class vectors to binary class matrices (one-hot encoding).\n",
        "  y_train = to_categorical(y_train, n_classes)\n",
        "  y_test = to_categorical(y_test, n_classes)\n",
        "\n",
        "  # Be sure that your training/test data is 'float32'\n",
        "  x_train = ...\n",
        "  x_test = ...\n",
        "  # Be sure that your training/test data are between 0 and 1 (pixel image value)\n",
        "  x_train ...\n",
        "  x_test ...\n",
        "\n",
        "  try:\n",
        "    # Fit with keras using 'datagen', the previously defined image generator\n",
        "    history = model.fit_generator(...)\n",
        "  except KeyboardInterrupt:\n",
        "    print(\"Training interrupted!\")\n",
        "  \n",
        "  # first, save the model\n",
        "  json_str = model.to_json()\n",
        "  model_path = os.path.join(OUTDIR, model_filename)\n",
        "  weight_path = os.path.join(OUTDIR, weight_filename)\n",
        "  with open(model_path, \"w\") as txtfile:\n",
        "    txtfile.write(json_str)\n",
        "  \n",
        "  # then, save the weights\n",
        "  model.save_weights(weight_path)\n",
        "  \n",
        "  # finally, plot and save the metrics\n",
        "  ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aXV-9BxNT59c"
      },
      "outputs": [],
      "source": [
        "# fit your model\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxaUv7qk-Zr"
      },
      "source": [
        "# EX2: Lenet-like auto-encoder.\n",
        "\n",
        "This one is very hard, It is fine if you do not finish.\n",
        "It's about autoencoders, feel free to read more about autoencoders : https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "Autoencoders are a very special kind of model. Input $x$ is an image, output $y$ is an image. The model is supposed to predict $x = y$ ! Usually, you build an autoencoder to create a 'deep' representation of your images : the model break your image into a high level descriptor that is useful to build your image back (the descriptors' space is supposed to be very powerful for clustering and image matching algorithms). The representation is built in an unsupervised way ; if $f$ is the function defined by your model, then you expect $f$ to be such as:\n",
        "$$f(x) = y;$$\n",
        "$$x = y$$\n",
        "So you try to minimize $||f(x) - x||^2$ (mean squared error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_IaGLkclOmM"
      },
      "source": [
        "## 1- Build model architecture\n",
        "The architecture has to be a CLASS called \"Autoencoder\".\n",
        "\n",
        "Basically, an autoencoder has two parts:\n",
        "- the Encoder $(conv2D-Maxpooling)\\times n$\n",
        "- the Decoder $(conv2D-Upsampling)\\times n$\n",
        "\n",
        "\n",
        "Autoencoder has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n",
        "\n",
        "------------------------------\n",
        "\n",
        "For the Encoder:\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n",
        "\n",
        "------------------------------\n",
        "For the Decoder:\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to increase the spatial size by a factor of 2 in each direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mCG3NvJJpy8B"
      },
      "outputs": [],
      "source": [
        "from keras.layers import BatchNormalization, UpSampling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hJ8Io8DFp0sI"
      },
      "outputs": [],
      "source": [
        "OUTDIR = \"/content/drive/My Drive/Autoencoding\"\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.makedirs(OUTDIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L2XEJ0oTESRR"
      },
      "outputs": [],
      "source": [
        "# first model\n",
        "# Lenet-like\n",
        "class Autoencoder:\n",
        "  \"\"\"\n",
        "  Aurtoencoder architecture.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Architecture settings.\n",
        "    \"\"\"\n",
        "    # nothing to do in the init.\n",
        "  \n",
        "  def __call__(self, X):\n",
        "    \"\"\"\n",
        "    Call autoencoder layers on the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    # encode\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    # decode\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyQr2CKeqk8b"
      },
      "source": [
        "## 2- A function to create a model with your Autoencoder architecture\n",
        "I want the model to be able to fit on the following datasets:\n",
        "\n",
        "- cifar-10\n",
        "\n",
        "Create a function called \"make_autoencoder_model\" that take no argument.\n",
        "It only returns a keras Model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tiF9FQEBqbHB"
      },
      "outputs": [],
      "source": [
        "def make_autoencoder_model():\n",
        "  \"\"\"\n",
        "  Create and compile autoencoder keras model.\n",
        "  \"\"\"\n",
        "  X = Input(batch_shape=(...))\n",
        "  Y = Autoencoder()(X)\n",
        "  model = Model(inputs=X, outputs=Y)\n",
        "  model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIk0P030rHwK"
      },
      "source": [
        "Useful function to plot the resulting image produced by your autoencoder.\n",
        "Read them carefully if you want to plot your output correctly (especially 'comparison')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kk8X5Vm4rGEr"
      },
      "outputs": [],
      "source": [
        "def stack_horizontally(min, max, images):\n",
        "    return numpy.hstack(images[i] for i in range(min, max))\n",
        "\n",
        "def stack_vertically(length, height, images):\n",
        "    return numpy.vstack(stack_horizontally(i * length, (i + 1) * length, images) for i in range(height))\n",
        "\n",
        "def comparison(inputimgs, outputimgs, length, height):\n",
        "    A = stack_vertically(length, height, inputimgs)\n",
        "    B = stack_vertically(length, height, outputimgs)\n",
        "    C = numpy.ones((A.shape[0], 32, 3))\n",
        "    return numpy.hstack((A, C, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muPRC6hfseSH"
      },
      "source": [
        "## 3- Create a fitting function\n",
        "\n",
        "I want a function \"fit_model_cifar10\" with the following arguments:\n",
        "\n",
        "- dataset: str name of the dataset\n",
        "- n_epochs: number of times you fit on the entire training set\n",
        "- batch_size: number of images to average gradient on\n",
        "- visualization_size (square root of number of test pred you wanna show to check your result)\n",
        "- verbose (debug purpose to see tensor dimensions in your architecture)\n",
        "\n",
        "The function must create an autoencoder model and fit it following these parameters.\n",
        "The function should:\n",
        "\n",
        "- fit the model, obviously\n",
        "- store the model architecture in a .json file in your output directory\n",
        "- store the model's weights in a .h5 file in your output directory\n",
        "- store the fitting metrics loss, validation_loss, under the form of a plot exported in a png file.\n",
        "- store the rebuilt test images under the form of a plot exported in a png file.\n",
        "\n",
        "Run your fitting function of course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yXeUGocrRDp"
      },
      "outputs": [],
      "source": [
        "def fit_model_on_cifar10(n_epochs=3, batch_size=128, visualization_size=5, verbose=1):\n",
        "\n",
        "  model_filename = \"autoencoder_cifar10.json\"\n",
        "  weight_filename = \"autoencoder_cifar10_weights.h5\"\n",
        "  lossplot_filename = \"autoencoder_cifar10_loss.png\"\n",
        "  visuplot_filename = \"autoencoder_cifar10_visu.png\"\n",
        "  # create your model and call it on your dataset\n",
        "  model = ...\n",
        "  if verbose > 0:\n",
        "    print(model.summary())\n",
        "  (x_train, _), (x_test, _) = cifar10.load_data()\n",
        "  # Be sure that your training/test data is 'float32'\n",
        "  x_train = ...\n",
        "  x_test = ...\n",
        "  # Be sure that your training/test data are between 0 and 1 (pixel image value)\n",
        "  x_train ...\n",
        "  x_test ...\n",
        "  try:\n",
        "    history = model.fit(...)\n",
        "  except KeyboardInterrupt:\n",
        "    print(\"Training interrupted!\")\n",
        "  \n",
        "  # first, save the model\n",
        "  json_str = model.to_json()\n",
        "  model_path = os.path.join(OUTDIR, model_filename)\n",
        "  weight_path = os.path.join(OUTDIR, weight_filename)\n",
        "  with open(model_path, \"w\") as txtfile:\n",
        "    txtfile.write(json_str)\n",
        "  # then, save the weights\n",
        "  model.save_weights(weight_path)\n",
        "  # now, save metrics plots\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQEtpB9rsU_2"
      },
      "outputs": [],
      "source": [
        "# fit your model\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Notebook_6.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}